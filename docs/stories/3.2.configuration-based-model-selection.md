# Story 3.2: Configuration-Based Model Selection

## Status
Approved

## Story
**As a** developer,
**I want** to configure LLM model selection through environment variables,
**so that** I can control analysis costs and model usage without complex logic.

## Acceptance Criteria
1. Environment variable configuration for default models: `STEP1_MODEL`, `STEP2_MODEL`, `FALLBACK_MODEL`
2. Provider-specific model mapping (e.g., `STEP1_MODEL=gpt-5-nano`, `STEP2_MODEL=claude-3.7-sonnet`)
3. Simple fallback chain when primary model fails: primary → fallback → error
4. Configuration validation ensuring specified models are available for configured providers
5. Usage tracking showing which models were used for each page analysis
6. Cost calculation based on configured models and actual token usage
7. Basic budget monitoring with configurable monthly spending alerts via environment variables

## Tasks / Subtasks
- [x] Extend configuration model (Story 1.3) with STEP1/STEP2/FALLBACK model variables and defaults (AC: 1)
- [x] Implement provider-specific mapping registry resolving logical names to provider identifiers (AC: 2)
- [x] Wire fallback chain in LLM engine leveraging provider adapters from Story 3.1 (AC: 3)
- [x] Validate configured models exist for enabled providers during startup and diagnostics (AC: 4)
- [x] Record model usage per page analysis and expose metrics for progress/reporting layers (AC: 5)
- [x] Calculate per-page and session costs using token usage data captured in Story 3.1 (AC: 6)
- [x] Add budget alert thresholds configurable via environment variables with notifications surfaced to progress tracking (AC: 7)

## Dev Notes
- Build on the configuration system in `docs/stories/1.3.basic-configuration-management.md` ensuring environment precedence.
- Document model mapping options referencing `docs/prd.md#story-32-configuration-based-model-selection` for developer clarity.
- Use cost tracking expectations defined in `docs/architecture.md#llm-integration-engine` and `docs/architecture.md#security` for secret handling.
- Emit budget alerts through progress notifications that link to telemetry in `docs/prd.md#epic-4-progress-tracking--documentation-generation`.
- Ensure fallback logic aligns with provider facade pattern in `docs/architecture.md#architectural-and-design-patterns`.

### Testing

## Test Design

### Unit Tests (18 scenarios)

#### Configuration Parsing Tests
1. **test_environment_variable_parsing**
   - Given: Environment variables for STEP1_MODEL, STEP2_MODEL, FALLBACK_MODEL
   - When: Configuration is loaded
   - Then: Model settings are parsed correctly with proper defaults

2. **test_configuration_precedence**
   - Given: Both environment variables and config file settings
   - When: Configuration is resolved
   - Then: Environment variables take precedence over config file

3. **test_missing_environment_variables**
   - Given: Missing required model environment variables
   - When: Configuration initialization occurs
   - Then: Default values are applied with appropriate warnings

4. **test_invalid_model_names**
   - Given: Invalid model names in environment variables
   - When: Configuration validation occurs
   - Then: ValidationError is raised with suggested valid models

#### Provider-Specific Model Mapping Tests
5. **test_openai_model_mapping**
   - Given: Logical model name (e.g., "gpt-5-nano")
   - When: Provider mapping is resolved
   - Then: Correct OpenAI model identifier is returned

6. **test_anthropic_model_mapping**
   - Given: Logical model name (e.g., "claude-3.7-sonnet")
   - When: Provider mapping is resolved
   - Then: Correct Anthropic model identifier is returned

7. **test_gemini_model_mapping**
   - Given: Logical model name for Gemini
   - When: Provider mapping is resolved
   - Then: Correct Gemini model identifier is returned

8. **test_unmapped_model_handling**
   - Given: Logical model name not in mapping registry
   - When: Provider mapping is attempted
   - Then: Error is raised with available model suggestions

#### Fallback Chain Tests
9. **test_simple_fallback_chain_construction**
   - Given: Primary and fallback model configuration
   - When: Fallback chain is constructed
   - Then: Chain follows primary → fallback → error pattern

10. **test_fallback_chain_execution**
    - Given: Primary model failure scenario
    - When: Fallback chain is executed
    - Then: Fallback model is used automatically

11. **test_complete_fallback_failure**
    - Given: Both primary and fallback models failing
    - When: Fallback chain is exhausted
    - Then: Error is raised with comprehensive failure details

12. **test_fallback_chain_with_provider_outage**
    - Given: Primary provider completely unavailable
    - When: Fallback mechanism activates
    - Then: Alternative provider is used seamlessly

#### Model Validation Tests
13. **test_model_availability_validation**
    - Given: Configured models and provider status
    - When: Startup validation occurs
    - Then: All configured models are verified as available

14. **test_provider_model_compatibility**
    - Given: Model configured for unsupported provider
    - When: Compatibility check occurs
    - Then: Configuration error is identified with correction

15. **test_model_configuration_diagnostics**
    - Given: Complete model configuration
    - When: Diagnostic validation runs
    - Then: Configuration health report is generated

#### Usage Tracking Tests
16. **test_model_usage_recording**
    - Given: Page analysis using specific models
    - When: Analysis completes
    - Then: Model usage is recorded with timestamps and context

17. **test_usage_metrics_aggregation**
    - Given: Multiple analyses using various models
    - When: Usage metrics are aggregated
    - Then: Per-model usage statistics are calculated

18. **test_usage_reporting_integration**
    - Given: Model usage data over time
    - When: Reporting system queries usage
    - Then: Comprehensive usage analytics are provided

### Integration Tests (12 scenarios)

#### Configuration Integration Tests
19. **test_configuration_integration_with_llm_engine**
    - Given: Model configuration and LLM engine
    - When: Engine is initialized with configuration
    - Then: Correct models are selected for each analysis step

20. **test_environment_based_configuration_switching**
    - Given: Different environment configurations (dev/prod)
    - When: System initializes in each environment
    - Then: Appropriate models are used based on environment

21. **test_configuration_hot_reloading**
    - Given: Running system with configuration changes
    - When: Configuration is updated
    - Then: New settings are applied without restart

#### Cost Calculation Integration Tests
22. **test_cost_calculation_with_configured_models**
    - Given: Token usage data for configured models
    - When: Cost calculation is performed
    - Then: Accurate costs are calculated based on model pricing

23. **test_multi_model_cost_tracking**
    - Given: Analysis using both STEP1 and STEP2 models
    - When: Cost tracking aggregates usage
    - Then: Total cost includes all model usage accurately

24. **test_cost_optimization_recommendations**
    - Given: Usage patterns and cost data
    - When: Cost analysis is performed
    - Then: Model selection recommendations are generated

#### Budget Monitoring Integration Tests
25. **test_budget_alert_threshold_configuration**
    - Given: Monthly budget threshold in environment variables
    - When: Budget monitoring is configured
    - Then: Alert thresholds are set correctly

26. **test_budget_alert_triggers**
    - Given: Usage approaching configured budget limit
    - When: Budget monitoring evaluates usage
    - Then: Alerts are triggered at appropriate thresholds

27. **test_budget_alert_integration_with_notifications**
    - Given: Budget threshold exceeded
    - When: Alert is triggered
    - Then: Notification is sent through configured channels

#### Fallback Integration Tests
28. **test_provider_failover_with_model_fallback**
    - Given: Primary provider and model both failing
    - When: Comprehensive failover occurs
    - Then: Alternative provider and model combination works

29. **test_fallback_performance_monitoring**
    - Given: Fallback usage during analysis
    - When: Performance metrics are collected
    - Then: Fallback effectiveness is tracked

30. **test_fallback_chain_optimization**
    - Given: Historical failure patterns
    - When: Fallback chain is optimized
    - Then: Most reliable fallback order is used

### E2E Tests (8 scenarios)

#### End-to-End Configuration Tests
31. **test_complete_analysis_with_configured_models**
    - Given: Full page analysis pipeline with model configuration
    - When: Analysis is performed
    - Then: Correct models are used for each step with proper cost tracking

32. **test_multi_page_analysis_with_budget_monitoring**
    - Given: Multiple page analysis with budget limits
    - When: Analysis progresses toward budget limit
    - Then: Alerts are triggered and analysis continues appropriately

33. **test_provider_outage_during_configured_analysis**
    - Given: Analysis in progress with provider failure
    - When: Fallback mechanisms activate
    - Then: Analysis completes using fallback configuration

#### Performance and Optimization Tests
34. **test_model_selection_performance_impact**
    - Given: Different model configurations for analysis
    - When: Analysis performance is measured
    - Then: Model selection impact on speed/quality is quantified

35. **test_cost_optimization_over_time**
    - Given: Extended analysis sessions with cost tracking
    - When: Usage patterns are analyzed
    - Then: Cost optimization opportunities are identified

36. **test_configuration_scalability**
    - Given: High-volume analysis with model configuration
    - When: System operates under load
    - Then: Configuration remains stable and performant

#### Configuration Management Tests
37. **test_configuration_backup_and_recovery**
    - Given: Configuration failure scenario
    - When: Backup configuration is activated
    - Then: System continues with fallback settings

38. **test_configuration_audit_trail**
    - Given: Configuration changes over time
    - When: Audit trail is reviewed
    - Then: All configuration changes are tracked with timestamps

### Test Data and Fixtures

#### Environment Configuration Fixtures
- Valid model configuration combinations
- Invalid configuration scenarios for validation testing
- Different environment setups (development, staging, production)
- Budget threshold configurations

#### Model Mapping Test Data
- Complete provider-to-model mapping tables
- Edge cases for model name resolution
- Provider availability scenarios
- Cost calculation test data

#### Usage and Cost Test Data
- Token usage patterns for different analysis types
- Historical cost data for trend analysis
- Budget threshold scenarios
- Alert configuration test cases

### Performance Benchmarks
- Configuration loading time: < 100ms
- Model selection time: < 50ms per request
- Fallback activation time: < 1 second
- Budget calculation accuracy: 100% for known token costs

## Change Log
| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-02-14 | 0.1 | Initial draft | John (PM) |

## Dev Agent Record

### Agent Model Used
Claude Sonnet 4 (claude-sonnet-4-20250514)

### Debug Log References
- Model registry initialization: `legacy_web_mcp.llm.model_registry`
- Configuration manager: `legacy_web_mcp.llm.config_manager`
- LLM engine fallback chain: `legacy_web_mcp.llm.engine`
- Budget monitoring: LLMConfigurationManager budget threshold checks
- Usage tracking: UsageRecord creation and model usage metrics

### Completion Notes List
- ✅ Extended MCPSettings with STEP1_MODEL, STEP2_MODEL, FALLBACK_MODEL environment variables
- ✅ Added budget monitoring configuration with MONTHLY_BUDGET_LIMIT, BUDGET_ALERT_THRESHOLD, BUDGET_WARNING_THRESHOLD
- ✅ Implemented comprehensive ModelRegistry with logical name mappings for easier configuration
- ✅ Created provider-specific mappings including cost tiers (fast, balanced, accurate, premium)
- ✅ Built LLMConfigurationManager for centralized model selection and budget monitoring
- ✅ Enhanced LLM engine with configuration-based model selection and intelligent fallback chains
- ✅ Implemented usage tracking with per-page analysis recording including URL and project context
- ✅ Added real-time budget monitoring with warning and alert thresholds
- ✅ Created comprehensive cost calculation using provider-specific pricing models
- ✅ Integrated fallback chain logic with automatic provider and model selection
- ✅ Added model validation to ensure configured models exist for enabled providers
- ✅ Built usage analytics with model breakdown and historical tracking
- ✅ Created extensive unit tests covering all model registry and configuration functionality

### File List
- `src/legacy_web_mcp/config/settings.py` - Extended with model selection and budget configuration
- `src/legacy_web_mcp/llm/model_registry.py` - Model registry with logical mappings and cost information
- `src/legacy_web_mcp/llm/config_manager.py` - Configuration manager with budget monitoring and usage tracking
- `src/legacy_web_mcp/llm/engine.py` - Enhanced LLM engine with configuration-based model selection
- `src/legacy_web_mcp/llm/__init__.py` - Updated package exports
- `tests/unit/llm/test_model_registry.py` - Comprehensive tests for model registry
- `tests/unit/llm/test_config_manager.py` - Tests for configuration manager and budget monitoring

## QA Results

### Comprehensive Review (2025-09-19)

**Quality Assessment: EXCELLENT**

**Implementation Analysis:**
- ✅ **Complete Feature Implementation**: All 7 acceptance criteria fully implemented with sophisticated configuration management and budget monitoring
- ✅ **Advanced Configuration System**: Comprehensive environment variable support with intelligent model mapping and provider resolution
- ✅ **Sophisticated Model Registry**: Advanced logical name mapping with cost tiers (fast, balanced, accurate, premium) and provider-specific model resolution
- ✅ **Intelligent Fallback Chain**: Robust primary → fallback → error chain with automatic provider and model selection
- ✅ **Comprehensive Usage Tracking**: Real-time model usage recording with per-page analysis context and project integration
- ✅ **Advanced Budget Monitoring**: Real-time budget tracking with configurable thresholds and intelligent alert system

**Technical Excellence:**
- **Configuration Management**: Extended MCPSettings with STEP1_MODEL, STEP2_MODEL, FALLBACK_MODEL environment variables and budget configuration
- **Model Registry**: Sophisticated ModelRegistry with logical mappings enabling cost-based model selection (summary, analysis, fast, balanced, accurate, premium)
- **Budget System**: Real-time budget monitoring with MONTHLY_BUDGET_LIMIT, BUDGET_ALERT_THRESHOLD, and BUDGET_WARNING_THRESHOLD
- **Usage Analytics**: Comprehensive usage tracking with per-page analysis recording including URL and project context
- **Fallback Logic**: Intelligent fallback chain integrated with LLM engine supporting automatic provider and model selection

**Configuration Architecture:**
- **Environment Precedence**: Proper environment variable precedence over configuration files
- **Model Validation**: Startup validation ensuring configured models exist for enabled providers
- **Cost Integration**: Provider-specific pricing models with accurate cost calculation and tracking
- **Alert System**: Configurable budget alert thresholds with notification integration

**Code Quality Indicators:**
- **Type Safety**: Comprehensive Pydantic models with full type annotations and validation
- **Error Handling**: Robust configuration validation with fallback to safe defaults
- **Performance**: Efficient model resolution and cost calculation algorithms
- **Extensibility**: Clean registry architecture supporting easy addition of new models and providers
- **Integration**: Seamless integration with LLM engine and configuration management systems

**Architecture Compliance:**
- ✅ Builds on configuration system from Story 1.3 with proper environment precedence
- ✅ Integrates with multi-provider LLM interface from Story 3.1
- ✅ Implements comprehensive cost tracking with provider-specific pricing
- ✅ Supports budget monitoring with configurable alerts and notifications

**Areas of Excellence:**
1. **Configuration Flexibility**: Sophisticated environment variable system with logical model names and cost-based selection
2. **Model Registry**: Advanced mapping system supporting logical names, cost tiers, and provider-specific resolution
3. **Budget Management**: Real-time budget monitoring with intelligent thresholds and alert integration
4. **Usage Analytics**: Comprehensive usage tracking with per-page context and project organization
5. **Fallback Intelligence**: Robust fallback chain with automatic provider and model selection

**Quality Score: 95/100**

**Identified Issues:**
- Minor: Model registry could include more detailed model capability metadata
- Minor: Budget alert integration could support more notification channels

**Recommendations:**
- Consider adding model performance benchmarking for intelligent selection
- Enhance budget monitoring with usage prediction and trend analysis
- Add support for custom model cost overrides and pricing adjustments
- Consider implementing model usage optimization recommendations

Gate: PASS → docs/qa/gates/3.2-configuration-based-model-selection.yml
