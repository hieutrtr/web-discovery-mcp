# Story 1.4: URL Discovery and Sitemap Parsing

## Status
Ready for Done

## Story
**As a** user,
**I want** to input a website URL and automatically discover its structure,
**so that** I can understand the scope of analysis and select specific pages to examine.

## Acceptance Criteria
1. MCP tool `discover_website` accepts URL input and returns organized site structure
2. Automatic sitemap.xml parsing and URL extraction with error handling for missing sitemaps
3. Robots.txt analysis to identify additional URLs and crawling restrictions
4. Manual crawling fallback with configurable depth limits for sites without sitemaps
5. URL categorization and filtering (pages vs. assets, internal vs. external links)
6. Generated URL inventory saved to project folder with metadata (titles, descriptions, estimated complexity)
7. Progress reporting during discovery process for large websites

## Tasks / Subtasks
- [x] Implement `discover_website` tool orchestrating discovery pipeline (AC: 1)
  - [x] Validate and sanitize input URL using architecture security rules (AC: 1)
- [x] Add sitemap parsing module (AC: 2)
  - [x] Fetch and parse sitemap.xml with robust error handling and logging (AC: 2)
- [x] Integrate robots.txt analysis (AC: 3)
  - [x] Respect crawl directives and augment URL list with allowed paths (AC: 3)
- [x] Build manual crawling fallback (AC: 4)
  - [x] Implement bounded BFS/DFS with configurable depth from configuration defaults (AC: 4)
- [x] Categorize and filter discovered URLs (AC: 5)
  - [x] Distinguish page types, dedupe, and annotate internal/external classification (AC: 5)
- [x] Persist URL inventory to project storage (AC: 6)
  - [x] Save structured JSON/YAML with metadata including title, description, complexity heuristics (AC: 6)
- [x] Emit progress updates during discovery (AC: 7)
  - [x] Stream status via MCP resource or tool notifications for long-running operations (AC: 7)

## Dev Notes
- Reuse the storage layout from `docs/prd.md#story-15-project-organization-and-file-structure` and the Project model in `docs/architecture.md#project` to keep discovery artifacts compatible.
- Utilize AsyncIO to parallelize fetching while honoring the rate-limiting guidance in `docs/architecture.md#performance--scalability`.
- Generate titles/descriptions from meta tags or heuristics now, noting that deeper enrichment arrives with Epic 3 (`docs/prd.md#epic-3-llm-integration--two-step-analysis-pipeline`).
- Maintain structured logging consistent with `docs/architecture.md#architectural-and-design-patterns` for troubleshooting.
- Enforce robots.txt compliance and avoid destructive actions per the security practices in `docs/architecture.md#security`.

### Testing
- Unit tests for sitemap/robots parsers using fixture HTML files in `tests/fixtures/discovery/`.
- Integration test executing `discover_website` against mocked HTTP responses via pytest-httpx.
- Include performance test ensuring depth limits enforce expected bounds to support NFR processing times.

## Change Log
| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-02-14 | 0.1 | Initial draft | John (PM) |
| 2025-09-19 | 0.2 | Completed implementation of discovery pipeline with all components | James (Dev) |

## Dev Agent Record

### Agent Model Used

claude-sonnet-4-20250514

### Debug Log References

- uv run ruff check && uv run ruff check --fix
- uv run mypy src tests
- uv run pytest tests/unit/discovery/ -v
- uv run pytest tests/unit/mcp/test_discovery.py -v

### Completion Notes List

- Implemented complete website discovery pipeline with sitemap, robots.txt, and crawling support
- Added `discover_website` MCP tool with URL validation and structured output
- Built sitemap parsing module with XML handling and URL extraction
- Integrated robots.txt analysis for compliance and additional URL discovery
- Implemented manual crawling fallback with configurable depth limits and link extraction
- Added URL categorization and filtering for internal/external pages and assets
- Implemented project storage integration with JSON/YAML persistence
- Added comprehensive progress reporting via MCP context logging
- Created full test coverage including unit tests for all discovery components
- Added MCP integration tests for tool registration and functionality
- Fixed all linting and type checking issues

### File List

- src/legacy_web_mcp/discovery/__init__.py
- src/legacy_web_mcp/discovery/crawler.py
- src/legacy_web_mcp/discovery/http.py
- src/legacy_web_mcp/discovery/models.py
- src/legacy_web_mcp/discovery/pipeline.py
- src/legacy_web_mcp/discovery/robots.py
- src/legacy_web_mcp/discovery/sitemap.py
- src/legacy_web_mcp/discovery/utils.py
- src/legacy_web_mcp/mcp/discovery.py
- tests/unit/discovery/test_crawler.py
- tests/unit/discovery/test_pipeline.py
- tests/unit/discovery/test_robots.py
- tests/unit/discovery/test_sitemap.py
- tests/unit/discovery/test_utils.py
- tests/unit/mcp/test_discovery.py
- docs/stories/1.4.url-discovery-and-sitemap-parsing.md

## QA Results

### Review Date: 2025-09-19

### Reviewed By: Quinn (Test Architect)

### Code Quality Assessment

Excellent implementation quality with comprehensive coverage of all acceptance criteria. The codebase demonstrates clean architecture, proper async/await patterns, strong type safety, and robust error handling. The modular design effectively separates concerns across discovery strategies (sitemap, robots.txt, crawling) with well-defined data models and clear API boundaries.

### Refactoring Performed

None required. The implementation already follows best practices with proper abstraction layers, dependency injection, and clean separation of concerns.

### Compliance Check

- Coding Standards: ✓ Full compliance with Python 3.11+ features, async patterns, type annotations, and structlog usage
- Project Structure: ✓ Proper module organization under legacy_web_mcp.discovery with clear separation
- Testing Strategy: ✓ Comprehensive unit tests covering all discovery components with mocked HTTP responses
- All ACs Met: ✓ All 7 acceptance criteria fully implemented and tested

### Improvements Checklist

- [x] All discovery strategies (sitemap, robots.txt, crawling) implemented correctly
- [x] Comprehensive test coverage with both positive and negative scenarios
- [x] Proper async/await patterns throughout the codebase
- [x] Strong type safety with mypy validation
- [x] Clean code structure following architectural patterns
- [x] Robust error handling with custom exceptions
- [x] Progress reporting via MCP context logging
- [x] URL validation and normalization with security considerations

### Security Review

Strong security posture with proper URL validation, scheme restrictions (http/https only), robots.txt compliance, and input sanitization. The implementation prevents malicious URL inputs and respects crawling directives.

### Performance Considerations

Efficient implementation with async HTTP fetching, configurable depth limits for crawling, proper timeout handling, and structured progress reporting for large websites. No performance concerns identified.

### Files Modified During Review

None - implementation quality required no modifications.

### Gate Status

Gate: PASS → docs/qa/gates/1.4-url-discovery-and-sitemap-parsing.yml
Risk profile: docs/qa/assessments/1.4-risk-20250919.md
NFR assessment: docs/qa/assessments/1.4-nfr-20250919.md

### Recommended Status

✓ Ready for Done - Implementation complete with full test coverage and compliance

Gate: PASS → docs/qa/gates/1.4-url-discovery-and-sitemap-parsing.yml

Gate: PASS → docs/qa/gates/1.4-url-discovery-and-sitemap-parsing.yml
