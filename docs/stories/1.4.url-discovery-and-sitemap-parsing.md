# Story 1.4: URL Discovery and Sitemap Parsing

## Status
Draft

## Story
**As a** user,
**I want** to input a website URL and automatically discover its structure,
**so that** I can understand the scope of analysis and select specific pages to examine.

## Acceptance Criteria
1. MCP tool `discover_website` accepts URL input and returns organized site structure
2. Automatic sitemap.xml parsing and URL extraction with error handling for missing sitemaps
3. Robots.txt analysis to identify additional URLs and crawling restrictions
4. Manual crawling fallback with configurable depth limits for sites without sitemaps
5. URL categorization and filtering (pages vs. assets, internal vs. external links)
6. Generated URL inventory saved to project folder with metadata (titles, descriptions, estimated complexity)
7. Progress reporting during discovery process for large websites

## Tasks / Subtasks
- [ ] Implement `discover_website` tool orchestrating discovery pipeline (AC: 1)
  - [ ] Validate and sanitize input URL using architecture security rules (AC: 1)
- [ ] Add sitemap parsing module (AC: 2)
  - [ ] Fetch and parse sitemap.xml with robust error handling and logging (AC: 2)
- [ ] Integrate robots.txt analysis (AC: 3)
  - [ ] Respect crawl directives and augment URL list with allowed paths (AC: 3)
- [ ] Build manual crawling fallback (AC: 4)
  - [ ] Implement bounded BFS/DFS with configurable depth from configuration defaults (AC: 4)
- [ ] Categorize and filter discovered URLs (AC: 5)
  - [ ] Distinguish page types, dedupe, and annotate internal/external classification (AC: 5)
- [ ] Persist URL inventory to project storage (AC: 6)
  - [ ] Save structured JSON/YAML with metadata including title, description, complexity heuristics (AC: 6)
- [ ] Emit progress updates during discovery (AC: 7)
  - [ ] Stream status via MCP resource or tool notifications for long-running operations (AC: 7)

## Dev Notes
- Reuse architecture storage pattern for project folders: `/project-name/discovery/` etc., ensuring compatibility with Epic 1.5 outputs.
- Utilize AsyncIO to parallelize URL fetching without overwhelming target site; incorporate rate limiting from architecture guidance.
- Titles/descriptions can leverage lightweight heuristics or meta tags; future LLM enrichment handled in later epics.
- Maintain structured logging of discovery steps for troubleshooting.
- Ensure crawling respects robots.txt and avoids destructive actions to align with security constraints.

### Testing
- Unit tests for sitemap/robots parsers using fixture HTML files in `tests/fixtures/discovery/`.
- Integration test executing `discover_website` against mocked HTTP responses via pytest-httpx.
- Include performance test ensuring depth limits enforce expected bounds to support NFR processing times.

## Change Log
| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-02-14 | 0.1 | Initial draft | John (PM) |

## Dev Agent Record

### Agent Model Used

### Debug Log References

### Completion Notes List

### File List

## QA Results
