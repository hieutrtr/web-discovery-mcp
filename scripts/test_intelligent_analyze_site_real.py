#!/usr/bin/env python3
"""
Test Script for Story 6.5: AI-Driven Site Analysis Workflow with REAL LLM

This script tests the intelligent_analyze_site tool with actual AI-driven workflow orchestration,
demonstrating the complete end-to-end natural language site analysis process.

Prerequisites:
1. Set up API keys for LLM integration (REQUIRED for AI functionality):
   export OPENAI_API_KEY="your-key-here"
   # OR
   export ANTHROPIC_API_KEY="your-key-here"
   # OR
   export GEMINI_API_KEY="your-key-here"

2. Ensure Playwright browsers are installed:
   uv run playwright install

3. Run from project root:
   python scripts/test_intelligent_analyze_site_real.py

This will make REAL website requests and LLM API calls. Cost: ~$0.20-1.00 depending on analysis scope.
"""

import asyncio
import sys
import json
import os
from pathlib import Path
from datetime import datetime
from typing import Dict, Any

# Add the src directory to the path for imports
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

try:
    from legacy_web_mcp.mcp.server import create_mcp
    from fastmcp import Context
    from unittest.mock import AsyncMock
except ImportError as e:
    print(f"‚ùå Import error: {e}")
    print("Please ensure you're running from the project root and dependencies are installed.")
    sys.exit(1)


class IntelligentAnalysisTester:
    """Test runner for real AI-driven site analysis workflow."""

    def __init__(self):
        self.results = []

    def check_prerequisites(self):
        """Check if all prerequisites are met."""
        print("üîç Checking Prerequisites...")

        # Check API keys (REQUIRED for AI functionality)
        has_openai = bool(os.getenv("OPENAI_API_KEY"))
        has_anthropic = bool(os.getenv("ANTHROPIC_API_KEY"))
        has_gemini = bool(os.getenv("GEMINI_API_KEY"))
        has_llm = has_openai or has_anthropic or has_gemini

        print(f"   LLM API Keys: OpenAI={has_openai}, Anthropic={has_anthropic}, Gemini={has_gemini}")

        if not has_llm:
            print("   ‚ùå No LLM API keys found - AI functionality REQUIRES an API key!")
            print("   Please set one of: OPENAI_API_KEY, ANTHROPIC_API_KEY, or GEMINI_API_KEY")
            return False
        else:
            print("   ‚úÖ LLM integration available for AI analysis")

        # Check playwright browsers
        try:
            from playwright import async_api
            print("   ‚úÖ Playwright available")
        except ImportError:
            print("   ‚ùå Playwright not available")
            return False

        return True

    async def test_e_commerce_rebuild_analysis(self):
        """Test AI analysis for e-commerce site rebuilding."""
        print("\nüõí Testing E-Commerce Rebuild Analysis")
        print("=" * 60)

        server = create_mcp()
        tools = await server.get_tools()
        ai_tool = tools.get("intelligent_analyze_site")

        if not ai_tool:
            print("‚ùå intelligent_analyze_site tool not found!")
            return

        print("‚úÖ intelligent_analyze_site tool loaded")

        # Test e-commerce analysis request
        test_url = "https://example.com"
        natural_request = "Analyze this e-commerce site for complete rebuilding with modern React and Node.js. Focus on identifying the shopping cart, product catalog, and payment integration patterns."

        user_preferences = {
            "budget": "medium",
            "timeline": "6-8 months",
            "priorities": ["user experience", "mobile responsiveness", "performance"],
            "technology_stack": "React, Node.js, PostgreSQL"
        }

        print(f"üìã Test Configuration:")
        print(f"   URL: {test_url}")
        print(f"   Request: {natural_request}")
        print(f"   User Preferences: {json.dumps(user_preferences, indent=2)}")

        mock_context = AsyncMock()

        print(f"\nü§ñ Starting AI-driven e-commerce analysis...")
        print(f"   ‚ö†Ô∏è  This will make real web requests and LLM API calls")

        start_time = datetime.now()

        try:
            result = await ai_tool.fn(
                context=mock_context,
                natural_language_request=natural_request,
                url=test_url,
                user_preferences=json.dumps(user_preferences),
                project_id="ecommerce-rebuild-analysis"
            )

            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()

            print(f"\n‚è±Ô∏è  AI analysis completed in {duration:.2f} seconds")

            await self._analyze_ai_results(result, "E-Commerce Rebuild")

        except Exception as e:
            print(f"\nüí• Exception during e-commerce analysis: {e}")
            import traceback
            print(f"Traceback: {traceback.format_exc()}")

    async def test_legacy_cms_assessment(self):
        """Test AI analysis for legacy CMS assessment."""
        print("\n\nüìÑ Testing Legacy CMS Assessment")
        print("=" * 60)

        server = create_mcp()
        tools = await server.get_tools()
        ai_tool = tools.get("intelligent_analyze_site")

        test_url = "https://httpbin.org"
        natural_request = "Assess this legacy content management system for modernization. I need to understand the content structure, admin functionality, and migration complexity."

        user_preferences = {
            "urgency": "high",
            "focus_areas": ["content migration", "admin workflows", "SEO preservation"],
            "constraints": ["minimal downtime", "preserve existing URLs"]
        }

        print(f"üìã Test Configuration:")
        print(f"   URL: {test_url}")
        print(f"   Request: {natural_request}")
        print(f"   User Preferences: {json.dumps(user_preferences, indent=2)}")

        mock_context = AsyncMock()

        print(f"\nüß† Starting AI-driven CMS assessment...")

        try:
            result = await ai_tool.fn(
                context=mock_context,
                natural_language_request=natural_request,
                url=test_url,
                user_preferences=json.dumps(user_preferences),
                project_id="cms-modernization-assessment"
            )

            await self._analyze_ai_results(result, "Legacy CMS Assessment")

        except Exception as e:
            print(f"\nüí• Exception during CMS assessment: {e}")

    async def test_security_audit_workflow(self):
        """Test AI analysis for security audit workflow."""
        print("\n\nüîí Testing Security Audit Workflow")
        print("=" * 60)

        server = create_mcp()
        tools = await server.get_tools()
        ai_tool = tools.get("intelligent_analyze_site")

        test_url = "https://httpbin.org/forms/post"
        natural_request = "Conduct a comprehensive security audit of this web application. Focus on authentication, data handling, and potential vulnerabilities."

        user_preferences = {
            "audit_depth": "comprehensive",
            "compliance_requirements": ["GDPR", "SOC2"],
            "security_priorities": ["authentication", "data protection", "input validation"]
        }

        print(f"üìã Test Configuration:")
        print(f"   URL: {test_url}")
        print(f"   Request: {natural_request}")
        print(f"   User Preferences: {json.dumps(user_preferences, indent=2)}")

        mock_context = AsyncMock()

        print(f"\nüõ°Ô∏è  Starting AI-driven security audit...")

        try:
            result = await ai_tool.fn(
                context=mock_context,
                natural_language_request=natural_request,
                url=test_url,
                user_preferences=json.dumps(user_preferences),
                project_id="security-audit-workflow"
            )

            await self._analyze_ai_results(result, "Security Audit")

        except Exception as e:
            print(f"\nüí• Exception during security audit: {e}")

    async def test_performance_optimization_analysis(self):
        """Test AI analysis for performance optimization."""
        print("\n\n‚ö° Testing Performance Optimization Analysis")
        print("=" * 60)

        server = create_mcp()
        tools = await server.get_tools()
        ai_tool = tools.get("intelligent_analyze_site")

        test_url = "https://example.com"
        natural_request = "Analyze this site for performance optimization opportunities. I want to improve page load times and Core Web Vitals scores."

        print(f"üìã Test Configuration:")
        print(f"   URL: {test_url}")
        print(f"   Request: {natural_request}")
        print(f"   User Preferences: None (testing defaults)")

        mock_context = AsyncMock()

        print(f"\nüöÄ Starting AI-driven performance analysis...")

        try:
            result = await ai_tool.fn(
                context=mock_context,
                natural_language_request=natural_request,
                url=test_url,
                project_id="performance-optimization-analysis"
            )

            await self._analyze_ai_results(result, "Performance Optimization")

        except Exception as e:
            print(f"\nüí• Exception during performance analysis: {e}")

    async def test_ai_workflow_error_handling(self):
        """Test AI workflow error handling with edge cases."""
        print("\n\nüõ°Ô∏è  Testing AI Workflow Error Handling")
        print("=" * 60)

        server = create_mcp()
        tools = await server.get_tools()
        ai_tool = tools.get("intelligent_analyze_site")

        mock_context = AsyncMock()

        # Test 1: Invalid URL
        print("üîç Testing with invalid URL...")
        try:
            result = await ai_tool.fn(
                context=mock_context,
                natural_language_request="Analyze this site for issues",
                url="https://this-domain-definitely-does-not-exist-12345.com",
                project_id="error-handling-invalid-url"
            )

            if result.get("status") == "error":
                print("‚úÖ Invalid URL error handling working correctly!")
                print(f"   Error: {result.get('error', 'N/A')}")
            else:
                print(f"‚ö†Ô∏è  Unexpected result for invalid URL: {result.get('status', 'N/A')}")

        except Exception as e:
            print(f"‚úÖ Exception handling working: {e}")

        # Test 2: Malformed preferences
        print("\nüîç Testing with malformed user preferences...")
        try:
            result = await ai_tool.fn(
                context=mock_context,
                natural_language_request="Analyze this site",
                url="https://example.com",
                user_preferences="invalid json {{{",
                project_id="error-handling-malformed-prefs"
            )

            print("‚úÖ Malformed preferences handled gracefully")
            print(f"   Status: {result.get('status', 'N/A')}")

        except Exception as e:
            print(f"‚ö†Ô∏è  Exception with malformed preferences: {e}")

    async def _analyze_ai_results(self, result: Dict[str, Any], analysis_type: str):
        """Analyze and display AI-driven analysis results."""
        if result.get("status") == "success":
            print(f"\n‚úÖ AI {analysis_type.upper()} ANALYSIS SUCCESSFUL!")
            print("=" * 50)

            # AI Intent Analysis
            if 'analysis_intent' in result:
                intent = result['analysis_intent']
                print(f"\nüéØ AI Intent Recognition:")
                print(f"   Primary Intent: {intent.get('primary_intent', 'N/A')}")
                print(f"   Specific Goals: {intent.get('specific_goals', [])}")
                print(f"   Urgency Level: {intent.get('urgency_level', 'N/A')}")
                print(f"   Depth Preference: {intent.get('depth_preference', 'N/A')}")
                print(f"   AI Summary: {intent.get('summary', 'N/A')}")

            # Site Pattern Detection
            if 'site_pattern' in result:
                pattern = result['site_pattern']
                print(f"\nüèóÔ∏è  AI Site Pattern Detection:")
                print(f"   Detected Type: {pattern.get('type', 'N/A')}")
                print(f"   Confidence Level: {pattern.get('confidence', 0):.1%}")
                print(f"   Key Characteristics: {pattern.get('key_characteristics', [])}")
                print(f"   Recommended Approach: {pattern.get('recommended_analysis_approach', 'N/A')}")
                print(f"   Estimated Complexity: {pattern.get('estimated_complexity', 'N/A')}")

            # Intelligent Workflow Plan
            if 'workflow_plan' in result:
                plan = result['workflow_plan']
                print(f"\nüìã AI Workflow Planning:")
                print(f"   Analysis Mode: {plan.get('analysis_mode', 'N/A')}")
                print(f"   Cost Priority: {plan.get('cost_priority', 'N/A')}")
                print(f"   Max Pages: {plan.get('max_pages', 'N/A')}")
                print(f"   Include Step 2: {plan.get('include_step2', 'N/A')}")
                print(f"   Strategy Summary: {plan.get('strategy_summary', 'N/A')}")

            # Analysis Results
            if 'analysis_result' in result:
                analysis = result['analysis_result']
                print(f"\nüìä Analysis Execution:")
                print(f"   Status: {analysis.get('status', 'N/A')}")
                print(f"   Pages Analyzed: {analysis.get('pages_analyzed', 'N/A')}")
                print(f"   Total Pages Found: {analysis.get('total_pages_found', 'N/A')}")
                print(f"   Analysis Quality Score: {analysis.get('analysis_quality_score', 'N/A')}")

                if 'discovery_results' in analysis:
                    discovery = analysis['discovery_results']
                    print(f"   Discovery Method: {discovery.get('method', 'N/A')}")
                    if discovery.get('site_characteristics'):
                        site_chars = discovery['site_characteristics']
                        print(f"   Site Type: {site_chars.get('site_type', 'N/A')}")

            # AI-Powered Synthesis
            if 'synthesized_insights' in result:
                insights = result['synthesized_insights']
                print(f"\nüß† AI-Powered Insights Synthesis:")
                print(f"   Executive Summary: {insights.get('executive_summary', 'N/A')}")

                if insights.get('prioritized_findings'):
                    print(f"   Priority Findings:")
                    for i, finding in enumerate(insights['prioritized_findings'][:3], 1):
                        print(f"      {i}. {finding}")

                if insights.get('actionable_next_steps'):
                    print(f"   Next Steps:")
                    for i, step in enumerate(insights['actionable_next_steps'][:3], 1):
                        print(f"      {i}. {step}")

            # Learning and Adaptation
            if 'learning_metadata' in result:
                learning = result['learning_metadata']
                print(f"\nüéì AI Learning & Adaptation:")
                print(f"   Analysis Pattern: {learning.get('analysis_pattern', 'N/A')}")
                print(f"   Quality Assessment: {learning.get('quality_assessment', 'N/A')}")
                print(f"   Improvement Suggestions: {learning.get('improvement_suggestions', [])}")

            # Quality Assessment
            overall_quality = "N/A"
            if result.get('analysis_result', {}).get('analysis_quality_score'):
                quality_score = result['analysis_result']['analysis_quality_score']
                overall_quality = f"{quality_score:.1%}"

            print(f"\nüìà AI Analysis Quality Assessment:")
            print(f"   Overall Quality Score: {overall_quality}")
            print(f"   Workflow Status: {result.get('analysis_result', {}).get('workflow_status', 'N/A')}")

            if isinstance(result.get('analysis_result', {}).get('analysis_quality_score'), (int, float)):
                quality_score = result['analysis_result']['analysis_quality_score']
                if quality_score > 0.8:
                    print("   ‚úÖ High-quality AI analysis achieved!")
                elif quality_score > 0.5:
                    print("   ‚ö†Ô∏è  Medium-quality AI analysis - acceptable")
                else:
                    print("   ‚ùå Low-quality AI analysis - needs review")

        elif result.get("status") == "error":
            print(f"\n‚ùå AI {analysis_type} Analysis failed: {result.get('error', 'Unknown error')}")
            print(f"   Error type: {result.get('error_type', 'Unknown')}")
            print(f"   Request: {result.get('natural_language_request', 'N/A')}")

        else:
            print(f"\n‚ö†Ô∏è  Unexpected AI analysis status: {result.get('status', 'Unknown')}")

    async def run_intelligent_analysis_tests(self):
        """Run all AI-driven site analysis integration tests."""
        print("ü§ñ Story 6.5: AI-Driven Site Analysis Workflow Test Suite")
        print("=" * 70)
        print("This test demonstrates the intelligent_analyze_site tool with REAL AI-driven analysis.")
        print("‚ö†Ô∏è  WARNING: This will make real web requests and LLM API calls (costs money)")
        print()

        if not self.check_prerequisites():
            print("‚ùå Prerequisites not met. Please install missing dependencies and set API keys.")
            return

        # Check if user wants to proceed
        response = input("Do you want to proceed with AI-driven analysis tests? (y/N): ")
        if response.lower() != 'y':
            print("‚ùå Test cancelled by user")
            return

        print("\nüöÄ Starting AI-driven site analysis tests...")

        await self.test_e_commerce_rebuild_analysis()
        await self.test_legacy_cms_assessment()
        await self.test_security_audit_workflow()
        await self.test_performance_optimization_analysis()
        await self.test_ai_workflow_error_handling()

        print("\n" + "=" * 70)
        print("üéâ AI-Driven Site Analysis Tests Complete!")
        print("=" * 70)
        print("‚úÖ You have successfully tested Story 6.5 with REAL AI analysis")
        print("‚úÖ Natural language command parsing and intent recognition work")
        print("‚úÖ AI-powered site pattern detection is functional")
        print("‚úÖ Intelligent workflow planning and tool selection work")
        print("‚úÖ Adaptive analysis strategies are operational")
        print("‚úÖ AI-powered result synthesis with prioritized recommendations work")
        print("‚úÖ Conversational progress updates are functional")
        print("‚úÖ Learning and error handling capabilities are operational")
        print()
        print("üéØ Key Features Demonstrated:")
        print("   ‚Ä¢ Natural language analysis requests")
        print("   ‚Ä¢ AI-driven site pattern recognition")
        print("   ‚Ä¢ Intelligent workflow orchestration")
        print("   ‚Ä¢ Adaptive analysis strategies")
        print("   ‚Ä¢ AI-powered insights synthesis")
        print("   ‚Ä¢ Learning from analysis patterns")
        print()
        print("üöÄ Next Steps:")
        print("   ‚Ä¢ Try with different natural language requests")
        print("   ‚Ä¢ Test with complex user preferences")
        print("   ‚Ä¢ Monitor LLM costs and API usage")
        print("   ‚Ä¢ Experiment with different site types")


async def main():
    """Main test execution."""
    tester = IntelligentAnalysisTester()
    await tester.run_intelligent_analysis_tests()


if __name__ == "__main__":
    asyncio.run(main())